{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 題目"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Use LSTM & CNN model to classify MNIST dataset with at least 90%\n",
    "2. Use LSTM & CNN model to classify customized candlestick pattern (at least 3 classes)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "環境說明： python==3.7.1  keras=2.2.4 pyts==0.7.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 執行"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "所有檔案: mnist_lstm.py、mnist_cnn.py candlestick_cnn.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HW2. Use LSTM & CNN model to classify MNIST\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "總結:\n",
    "   1，通過lstm範例了解了流程，然後自己寫出lenet網絡，來 classify MNIST\n",
    "   2, confusion matrix 的重要作用： 可以明顯看出正確與錯誤分類，同時能夠看出錯誤與正確差\"多遠\"，然後進壹步\"對癥下藥\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 2.1 mnist_lstm.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 256)               291840    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                2570      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 294,410\n",
      "Trainable params: 294,410\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "60000/60000 [==============================] - 63s 1ms/step - loss: 0.4396 - acc: 0.8532 - val_loss: 0.1623 - val_acc: 0.9492\n",
      "LSTM test accuracy: 0.9492\n",
      "[[5710    1   15    1   13   52   70    1   34   26]\n",
      " [   2 6638   30   20    6    3    2   19   15    7]\n",
      " [  25   24 5567   25   36   28   10  143   84   16]\n",
      " [  25   21   91 5431    3  167    3   40  335   15]\n",
      " [   6    9    9    0 5323   15   44   15   66  355]\n",
      " [   5   14   13   13   11 5259   32    1   67    6]\n",
      " [  19   12    0    0   41   89 5731    0   24    2]\n",
      " [   3   19   18   15   11   14    0 6036   12  137]\n",
      " [  14   44    7   25    3  103   17    1 5623   14]\n",
      " [  28   11   11   10   24  138    6   43  106 5572]] \n",
      "\n",
      " [[ 946    0    1    0    3    8   14    1    4    3]\n",
      " [   0 1122    4    0    0    3    1    2    3    0]\n",
      " [   5    0  967    2    3    5    2   31   16    1]\n",
      " [   1    1   11  929    0   22    0    7   37    2]\n",
      " [   0    0    0    0  896    2   12    2   14   56]\n",
      " [   0    0    1    3    0  865    9    1   13    0]\n",
      " [   5    3    0    0   10   18  914    0    6    2]\n",
      " [   1    7    6    5    0    1    0  978    8   22]\n",
      " [   5    2    1    2    2   24    0    2  935    1]\n",
      " [   3    4    1    1    2   24    2    7   25  940]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import keras\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense, Activation, Conv2D, MaxPool2D, Dropout, Flatten\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "\n",
    "def lstm_preprocess(x_train, x_test, y_train, y_test, n_step, n_input, n_classes):\n",
    "    x_train = x_train.reshape(-1, n_step, n_input)\n",
    "    x_test = x_test.reshape(-1, n_step, n_input)\n",
    "    x_train = x_train.astype('float32')\n",
    "    x_test = x_test.astype('float32')\n",
    "    x_train /= 255\n",
    "    x_test /= 255\n",
    "    y_train = keras.utils.to_categorical(y_train, n_classes)\n",
    "    y_test = keras.utils.to_categorical(y_test, n_classes)\n",
    "    return (x_train, x_test, y_train, y_test)\n",
    "\n",
    "\n",
    "def lstm_model(n_input, n_step, n_hidden, n_classes):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(n_hidden, batch_input_shape=(None, n_step, n_input), unroll=True))\n",
    "    model.add(Dense(n_classes))\n",
    "    model.add(Activation('softmax'))\n",
    "    return model\n",
    "\n",
    "\n",
    "def trainning(model, x_train, y_train, x_test, y_test, \n",
    "              learning_rate, training_iters, batch_size):\n",
    "    adam = Adam(lr=learning_rate)\n",
    "    model.summary()\n",
    "    model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.fit(x_train, y_train,\n",
    "              batch_size=batch_size, epochs=training_iters,\n",
    "              verbose=1, validation_data=(x_test, y_test))\n",
    "\n",
    "def print_confusion_result(x_train, x_test, y_train, y_test, model):\n",
    "    # get train & test predictions\n",
    "    train_pred = model.predict_classes(x_train)\n",
    "    test_pred = model.predict_classes(x_test)\n",
    "    \n",
    "    # get train & test true labels\n",
    "    train_label = y_train\n",
    "    test_label =  y_test\n",
    "    \n",
    "    # confusion matrix\n",
    "    train_result_cm = confusion_matrix(train_label, train_pred, labels=range(10))\n",
    "    test_result_cm = confusion_matrix(test_label, test_pred, labels=range(10))\n",
    "    print(train_result_cm, '\\n'*2, test_result_cm)\n",
    "\n",
    "def mnist_lstm_main():\n",
    "    # training parameters\n",
    "    learning_rate = 0.001\n",
    "    training_iters = 1\n",
    "    batch_size = 128\n",
    "\n",
    "    # model parameters\n",
    "    n_input = 28\n",
    "    n_step = 28\n",
    "    n_hidden = 256\n",
    "    n_classes = 10\n",
    "\n",
    "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "    x_train, x_test, y_train_o, y_test_o = lstm_preprocess(x_train, x_test, y_train, y_test, n_step, n_input, n_classes)\n",
    "\n",
    "    model = lstm_model(n_input, n_step, n_hidden, n_classes)\n",
    "    trainning(model, x_train, y_train_o, x_test, y_test_o, learning_rate, training_iters, batch_size)\n",
    "    scores = model.evaluate(x_test, y_test_o, verbose=0)\n",
    "    print('LSTM test accuracy:', scores[1])\n",
    "    print_confusion_result(x_train, x_test, y_train, y_test, model)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    mnist_lstm_main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 2.2 mnist_cnn.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/1\n",
      "48000/48000 [==============================] - 74s 2ms/step - loss: 0.1724 - acc: 0.9456 - val_loss: 0.0541 - val_acc: 0.9833\n",
      "\n",
      "Testing\n",
      "10000/10000 [==============================] - 5s 494us/step\n",
      "\n",
      "test loss:  0.04306174347337801\n",
      "\n",
      "test accuracy:  0.9852\n",
      "[[5889    5    7    2    1    3    7    1    3    5]\n",
      " [   1 6710   10    1    1    0    3   10    5    1]\n",
      " [   4   18 5805   45    7    0    1   56   18    4]\n",
      " [   2    1    7 6061    0   19    1   16    9   15]\n",
      " [   1   12    7    2 5731    0   18    9    6   56]\n",
      " [   4    2    2   32    3 5334   16    2   18    8]\n",
      " [  15    7    2    2    2   15 5860    0   15    0]\n",
      " [   4   15    7    8    5    3    0 6193    6   24]\n",
      " [  10   33   15   33    6   15    5    7 5682   45]\n",
      " [   8    5    2   14   12   15    1   28    8 5856]] \n",
      "\n",
      " [[ 975    1    1    0    0    0    1    1    1    0]\n",
      " [   0 1132    1    0    0    0    1    1    0    0]\n",
      " [   0    3 1010    8    0    0    0    9    2    0]\n",
      " [   0    0    0 1002    0    3    0    3    2    0]\n",
      " [   0    0    3    0  963    0    1    1    3   11]\n",
      " [   2    1    0    6    0  878    2    1    1    1]\n",
      " [   7    3    1    1    1    4  939    0    2    0]\n",
      " [   1    3    4    2    0    0    0 1013    1    4]\n",
      " [   2    0    1    6    1    0    1    6  953    4]\n",
      " [   3    5    0    3    0    4    1    6    0  987]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Conv2D, MaxPooling2D, Flatten\n",
    "from keras.optimizers import SGD\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0' \n",
    " \n",
    "\n",
    "def lenet():\n",
    "    model = Sequential()\n",
    "    #first conv and pool\n",
    "    model.add(Conv2D(input_shape=(28, 28, 1), kernel_size=(5, 5), filters=20, activation='relu')) \n",
    "    model.add(MaxPooling2D(pool_size=(2,2), strides=2, padding='same'))\n",
    "    #second conv and pool\n",
    "    model.add(Conv2D(kernel_size=(5, 5), filters=50,  activation='relu', padding='same'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2), strides=2, padding='same'))\n",
    " \n",
    "    model.add(Flatten()) \n",
    "    model.add(Dense(500, activation='relu')) #fc1\n",
    "    model.add(Dense(10, activation='softmax')) #fc2\n",
    "    sgd = SGD(lr=0.01,decay=1e-6,momentum=0.9,nesterov=True) \n",
    "    model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy']) \n",
    "    return model \n",
    " \n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    #data\n",
    "    (X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "    X_train = X_train.reshape(-1, 28, 28, 1)\n",
    "    X_test = X_test.reshape(-1, 28, 28, 1)\n",
    "    X_train = X_train / 255 \n",
    "    X_test = X_test / 255\n",
    "    y_train = np_utils.to_categorical(y_train, num_classes=10) #label onehot化\n",
    "    y_test = np_utils.to_categorical(y_test, num_classes=10)\n",
    " \n",
    "    \n",
    "    #train and test\n",
    "    model = lenet() \n",
    "    print('Training')\n",
    "    history = model.fit(X_train, y_train, epochs=1, batch_size=32,validation_split=0.2) \n",
    "    print('\\nTesting')\n",
    "    text_loss, text_accuracy = model.evaluate(X_test, y_test) \n",
    " \n",
    "    print('\\ntest loss: ', text_loss)\n",
    "    print('\\ntest accuracy: ', text_accuracy)\n",
    "    \n",
    "    #confusion\n",
    "    \n",
    "    print_confusion_result(X_train, X_test, np.argmax(y_train, axis=1), np.argmax(y_test, axis=1), model)\n",
    " \n",
    "    \n",
    "    #save model\n",
    "    model.save('./model/lenet.h5')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HW3. Use CNN model to classify customized candlestick pattern (at least 3 classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "總結：\n",
    "    1,概述： 分為兩個部分，第壹個是把時間序列數據弄成矩陣形式，第二個是運用CNN模型。\n",
    "    2，GAFS：運用pyts庫來進行transform\n",
    "    3，覺得這樣很好，因為這樣輸入的數據，更像是直觀的股票形態，從而機器能夠更像人學習判斷。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* candlestick_train_cnn.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#把HW1的輸出數據，拿來使用，也就是我用了'EveningStar','ShootingStar','BearishHarami'這三個指標。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyts.image import GASF\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from keras import backend as K\n",
    "from keras import optimizers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Conv2D, Activation, MaxPool2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data and process to the dict like pickle shows\n",
    "def load_csv(path='./data/eurusd_2010_2017_1T_rulebase.csv'):\n",
    "    \n",
    "    #get X and Y\n",
    "    df=pd.read_csv(path)\n",
    "    cols=['low','high','close','open','EveningStar','ShootingStar','BearishHarami','None']\n",
    "    temp=df.loc[:,cols]\n",
    "    data_temp=temp[ (df[cols[-4]]+df[cols[-3]]+df[cols[-2]]) ==1 ]\n",
    "    index=data_temp.index.values\n",
    "\n",
    "    X1=temp.loc[:,cols[0]].values\n",
    "    X2=temp.loc[:,cols[1]].values\n",
    "    X3=temp.loc[:,cols[2]].values\n",
    "    X4=temp.loc[:,cols[3]].values\n",
    "    \n",
    "    #use GASF transform\n",
    "    image_size = 10\n",
    "    gasf = GASF(image_size)\n",
    "    print(\"the  data  processing's iter  start\")\n",
    "    for num,i in enumerate(index):\n",
    "        n=(num+1)%1000\n",
    "        if n==0:\n",
    "            print(\"the  data  processing's iter is \",num+1)\n",
    "        s=i-9\n",
    "        e=i+1\n",
    "        X_gasf1 = gasf.fit_transform(X1[s:e].reshape(1, -1))\n",
    "        X_gasf2 = gasf.fit_transform(X2[s:e].reshape(1, -1))\n",
    "        X_gasf3 = gasf.fit_transform(X3[s:e].reshape(1, -1))\n",
    "        X_gasf4 = gasf.fit_transform(X4[s:e].reshape(1, -1))\n",
    "    \n",
    "        if num==0:\n",
    "            A1=np.stack([X_gasf1,X_gasf2,X_gasf3,X_gasf4]).reshape(1,4,10,10)\n",
    "        else:\n",
    "            A2=np.stack([X_gasf1,X_gasf2,X_gasf3,X_gasf4]).reshape(1,4,10,10)\n",
    "            A1=np.concatenate([A1,A2])\n",
    "    print(\"the  data  processing's iter  over\")\n",
    "    \n",
    "    #get data\n",
    "    X=A1.reshape(A1.shape[0],10,10,4)\n",
    "    Y=data_temp.loc[:,cols[-4:-1]].values\n",
    "    train_x,test_x,train_y,test_y=train_test_split(X,Y,test_size=0.2,random_state=0)\n",
    "    data={}\n",
    "    data['train_x']=train_x\n",
    "    data['train_y']=train_y\n",
    "    data['test_x']=test_x\n",
    "    data['test_y']=test_y\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "#model\n",
    "def get_cnn_model(params):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(filters=32, kernel_size=(5,5), padding='same', activation='relu', input_shape=(10, 10, 4)))\n",
    "    model.add(Conv2D(filters=48, kernel_size=(5,5), padding='valid', activation='relu'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dense(84, activation='relu'))\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "    return model\n",
    "\n",
    "#train\n",
    "def train_model(params, data):\n",
    "    model = get_cnn_model(params)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=params['optimizer'], metrics=['accuracy'])\n",
    "    hist = model.fit(x=data['train_x'], y=data['train_y'],\n",
    "                     batch_size=params['batch_size'], epochs=params['epochs'], verbose=2)\n",
    "    return (model, hist)\n",
    "\n",
    "#show\n",
    "def print_result(data, model):\n",
    "    # get train & test pred-labels\n",
    "    train_pred = model.predict_classes(data['train_x'])\n",
    "    test_pred = model.predict_classes(data['test_x'])\n",
    "    # get train & test true-labels\n",
    "    train_label = np.argmax(data['train_y'],axis=1)\n",
    "    test_label = np.argmax(data['test_y'],axis=1)\n",
    "    # confusion matrix\n",
    "    train_result_cm = confusion_matrix(train_label, train_pred, labels=range(3))\n",
    "    test_result_cm = confusion_matrix(test_label, test_pred, labels=range(3))\n",
    "    print(train_result_cm, '\\n'*2, test_result_cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the  data  processing's iter  start\n",
      "the  data  processing's iter is  1000\n",
      "the  data  processing's iter is  2000\n",
      "the  data  processing's iter is  3000\n",
      "the  data  processing's iter is  4000\n",
      "the  data  processing's iter is  5000\n",
      "the  data  processing's iter  over\n",
      "Epoch 1/10\n",
      " - 2s - loss: 0.8502 - acc: 0.6026\n",
      "Epoch 2/10\n",
      " - 1s - loss: 0.4655 - acc: 0.8531\n",
      "Epoch 3/10\n",
      " - 1s - loss: 0.3168 - acc: 0.9028\n",
      "Epoch 4/10\n",
      " - 1s - loss: 0.2482 - acc: 0.9234\n",
      "Epoch 5/10\n",
      " - 1s - loss: 0.2129 - acc: 0.9303\n",
      "Epoch 6/10\n",
      " - 1s - loss: 0.1882 - acc: 0.9404\n",
      "Epoch 7/10\n",
      " - 1s - loss: 0.1670 - acc: 0.9493\n",
      "Epoch 8/10\n",
      " - 1s - loss: 0.1537 - acc: 0.9507\n",
      "Epoch 9/10\n",
      " - 1s - loss: 0.1352 - acc: 0.9579\n",
      "Epoch 10/10\n",
      " - 1s - loss: 0.1229 - acc: 0.9615\n",
      "CNN test accuracy: 0.952020202020202\n",
      "[[ 762   15   30]\n",
      " [  15 2139  139]\n",
      " [  18   15 1618]] \n",
      "\n",
      " [[170   1  10]\n",
      " [  2 539  29]\n",
      " [  9   6 422]]\n"
     ]
    }
   ],
   "source": [
    "PARAMS = {}\n",
    "PARAMS['classes'] = 3\n",
    "PARAMS['lr'] = 0.01\n",
    "PARAMS['epochs'] = 10\n",
    "PARAMS['batch_size'] = 64\n",
    "PARAMS['optimizer'] = optimizers.SGD(lr=PARAMS['lr'])\n",
    "\n",
    "\n",
    "# # load data & keras model\n",
    "data = load_csv()\n",
    "# train cnn model\n",
    "model, hist = train_model(PARAMS, data)\n",
    "# train & test result\n",
    "scores = model.evaluate(data['test_x'], data['test_y'], verbose=0)\n",
    "print('CNN test accuracy:', scores[1])\n",
    "print_result(data, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
